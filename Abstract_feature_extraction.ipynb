{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"CSI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_text_path_1 = \"/home1/tirthankar/btpfinal/dataset/\"+DATA+\"/train/positive_bib_part/\"\n",
    "bib_text_path_2 = \"/home1/tirthankar/btpfinal/dataset/\"+DATA+\"/train/negative_bib_part/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(x,path,word_document_list,file_name,prefix):\n",
    "    c = 0\n",
    "    for file in x:\n",
    "        c = c+1\n",
    "        if c%100 == 0:\n",
    "            print(c)\n",
    "        key = prefix+file\n",
    "        file_name.append(key)\n",
    "        with open(join(path,file),encoding=\"utf8\") as input_file:\n",
    "            data = json.load(input_file)\n",
    "            abstract = []\n",
    "            if data['metadata']['abstractText'] is not None:\n",
    "                abstract = nltk.sent_tokenize(data['metadata']['abstractText'].lower())\n",
    "                if len(abstract) > 25:\n",
    "                    abstract = abstract[:25]\n",
    "            word_document_list.append(abstract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(sentence):\n",
    "    with tf.Graph().as_default():\n",
    "        embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\")\n",
    "        messages = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "        output = embed(messages)\n",
    "        with tf.Session() as session:\n",
    "            session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "            embeddings = session.run(output, feed_dict={messages: sentence})\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# embed the input x and pad appropriately\n",
    "def embed(x):\n",
    "    lens = list(map(lambda i:len(i), x))\n",
    "    x = list(it.chain.from_iterable(x))\n",
    "    print('Total sentences to be embedded: {}'.format(len(x)))\n",
    "    emb = run(x)\n",
    "    print(\"embedding done\")\n",
    "    embedded = []\n",
    "    zero = [0]*512\n",
    "    ir = iter(emb)\n",
    "    for i, l in enumerate(lens):\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "        z = []\n",
    "        while len(z)<l:\n",
    "            z.append(next(ir).tolist())\n",
    "        embedded.append(z)\n",
    "    print(\"Adding zeros\")\n",
    "    embedded = np.array(list(zip(*list(it.zip_longest(*embedded, fillvalue = zero)))))\n",
    "   # print(embedded.shape)\n",
    "    return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences to be embedded: 354\n",
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "#for  jnca get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "file_name_final  =[]\n",
    "use_vector = []\n",
    "for i in listdir(bib_text_path_1):\n",
    "    x = listdir(bib_text_path_1+i)\n",
    "    prefix = 'pos_'\n",
    "    file_name = []\n",
    "    word_document_list = []\n",
    "    get_sentences(x,bib_text_path_1+i,word_document_list,file_name,prefix)\n",
    "    y = embed(word_document_list)\n",
    "    pickle.dump(file_name, open(DATA+\"_train_abstract_filename_\"+i+\".p\", \"wb\"))  # save it into a file named save.p\n",
    "    pickle.dump(y, open(DATA+\"_train_abstract_use_vector_\"+i+\".p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "    \n",
    "    print(i) \n",
    "    \n",
    "    #for comnet get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "    \n",
    "    \n",
    "for i in listdir(bib_text_path_2):\n",
    "    x = listdir(bib_text_path_2+i)\n",
    "    prefix = 'neg_'\n",
    "    file_name = []\n",
    "    word_document_list = []\n",
    "    get_sentences(x,bib_text_path_2+i,word_document_list,file_name,prefix)\n",
    "    y = embed(word_document_list)\n",
    "    pickle.dump(file_name, open(DATA+\"_train_abstract_filename_\"+i+\".p\", \"wb\"))  # save it into a file named save.p\n",
    "    pickle.dump(y, open(DATA+\"_train_abstract_use_vector_\"+i+\".p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "    print(i)\n",
    "    #for comnet get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_text_path_1 = \"/home/tirthankar/Ashish/btpfinal/dataset/\"+DATA+\"/test/positive_bib_part/\"\n",
    "bib_text_path_2 = \"/home/tirthankar/Ashish/btpfinal/dataset/\"+DATA+\"/test/negative_bib_part/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_final  =[]\n",
    "use_vector = []\n",
    "for i in listdir(bib_text_path_1):\n",
    "    x = listdir(bib_text_path_1+i)\n",
    "    prefix = 'pos_'\n",
    "    file_name = []\n",
    "    word_document_list = []\n",
    "    get_sentences(x,bib_text_path_1+i,word_document_list,file_name,prefix)\n",
    "    y = embed(word_document_list)\n",
    "    \n",
    "    pickle.dump(file_name, open(DATA+\"_test_abstract_filename_\"+i+\".p\", \"wb\"))  # save it into a file named save.p\n",
    "    pickle.dump(y, open(DATA+\"_test_abstract_use_vector_\"+i+\".p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "    print(i) \n",
    "    \n",
    "    #for comnet get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "    \n",
    "    \n",
    "for i in listdir(bib_text_path_2):\n",
    "    x = listdir(bib_text_path_2+i)\n",
    "    prefix = 'neg_'\n",
    "    file_name = []\n",
    "    word_document_list = []\n",
    "    get_sentences(x,bib_text_path_2+i,word_document_list,file_name,prefix)\n",
    "    y = embed(word_document_list)\n",
    "    \n",
    "    pickle.dump(file_name, open(DATA+\"_test_abstract_filename_\"+i+\".p\", \"wb\"))  # save it into a file named save.p\n",
    "    pickle.dump(y, open(DATA+\"_test_abstract_use_vector_\"+i+\".p\", \"wb\"))  # save it into a file named save.p\n",
    "    print(i)\n",
    "    \n",
    "    #for comnet get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
