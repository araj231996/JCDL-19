{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"ARTINT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_text_path_1 = \"/home1/tirthankar/btpfinal/dataset/\"+DATA+\"/train/positive_bib\"\n",
    "bib_text_path_2 = \"/home1/tirthankar/btpfinal/dataset/\"+DATA+\"/train/negative_bib\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    words = []  \n",
    "    w = word_tokenize(sentence)\n",
    "    w = [word.lower() for word in w]\n",
    "    w = [word for word in w if word.isalpha()]\n",
    "    stop_words = stopwords.words('english')\n",
    "    w = [word for word in w if not word in stop_words]\n",
    "    words.extend(w)\n",
    "    return words\n",
    "\n",
    "    \n",
    "def bagofwords(sentence, words):\n",
    "    bag = np.zeros(len(words))\n",
    "    for sw in sentence:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] = 1                \n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams(x,path,paper_title,bib_title,bib_venue,prefix):\n",
    "\n",
    "    for file in x:\n",
    "        key = prefix+file\n",
    "        bib_title[key] = \"\"\n",
    "        bib_venue[key] = \"\"\n",
    "        with open(join(path,file),encoding=\"utf8\") as input_file:\n",
    "            data = json.load(input_file)\n",
    "            if data['metadata']['title'] is not None:\n",
    "                paper_title[key] = data['metadata']['title']\n",
    "            else:\n",
    "                paper_title[key] = \"None\"\n",
    "            if data['metadata']['references'] is not None:\n",
    "                for ref in data['metadata']['references']:\n",
    "                    if ref['title'] is not None:\n",
    "                        bib_title[key] = bib_title[key]+ref['title']+\" \"\n",
    "                    else:\n",
    "                        bib_title[key] = \"None\"\n",
    "                    if ref['venue'] is not None:\n",
    "                        bib_venue[key] = bib_venue[key]+ref['venue']+\" \"\n",
    "                    else:\n",
    "                        bib_venue[key] = \"None\"\n",
    "            paper_title[key] = preprocess(paper_title[key])\n",
    "            bib_title[key] = preprocess(bib_title[key])\n",
    "            bib_venue[key] = preprocess(bib_venue[key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for  jnca get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "x = listdir(bib_text_path_1)\n",
    "prefix = 'pos_'\n",
    "paper_title = {}\n",
    "bib_title = {}\n",
    "bib_venue = {}\n",
    "get_unigrams(x,bib_text_path_1,paper_title,bib_title,bib_venue,prefix)\n",
    "#for comnet get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "x = listdir(bib_text_path_2)\n",
    "prefix = 'neg_'\n",
    "get_unigrams(x,bib_text_path_2,paper_title,bib_title,bib_venue,prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(vocabulary_name,document_unigram,prefix,x):\n",
    "    for file in x:\n",
    "        vocabulary_name = vocabulary_name + document_unigram[prefix+file]\n",
    "    return vocabulary_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vocabulary = []\n",
    "bib_title_vocabulary = []\n",
    "bib_venue_vocabulary = []\n",
    "\n",
    "x = listdir(bib_text_path_1)\n",
    "title_vocabulary = generate_vocabulary(title_vocabulary,paper_title,'pos_',x)\n",
    "bib_title_vocabulary = generate_vocabulary(bib_title_vocabulary,bib_title,'pos_',x)\n",
    "bib_venue_vocabulary = generate_vocabulary(bib_venue_vocabulary,bib_venue,'pos_',x)\n",
    "\n",
    "x = listdir(bib_text_path_2)\n",
    "title_vocabulary = generate_vocabulary(title_vocabulary,paper_title,'neg_',x)\n",
    "bib_title_vocabulary = generate_vocabulary(bib_title_vocabulary,bib_title,'neg_',x)\n",
    "bib_venue_vocabulary = generate_vocabulary(bib_venue_vocabulary,bib_venue,'neg_',x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title_vocabulary1 = sorted(list(set(title_vocabulary)))\n",
    "bib_title_vocabulary1 = sorted(list(set(bib_title_vocabulary)))\n",
    "bib_venue_vocabulary1 = sorted(list(set(bib_venue_vocabulary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in set(bib_title_vocabulary1):\n",
    "    c = c+1\n",
    "    if c % 100 == 0:\n",
    "        print(c)\n",
    "    if bib_title_vocabulary.count(i) < 6:\n",
    "        bib_title_vocabulary1.remove(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in set(bib_venue_vocabulary1):\n",
    "    c = c+1\n",
    "    if c % 100 == 0:\n",
    "        print(c)\n",
    "    if bib_venue_vocabulary.count(i) < 3:\n",
    "        bib_venue_vocabulary1.remove(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in set(title_vocabulary1):\n",
    "    c = c+1\n",
    "    if c % 100 == 0:\n",
    "        print(c)\n",
    "    if title_vocabulary.count(i) < 3:\n",
    "        title_vocabulary1.remove(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90661\n"
     ]
    }
   ],
   "source": [
    "print(len(bib_venue_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90661\n",
      "2426\n",
      "423734\n",
      "5319\n",
      "18874\n",
      "1255\n"
     ]
    }
   ],
   "source": [
    "print(len(bib_venue_vocabulary))\n",
    "print(len(bib_venue_vocabulary1))\n",
    "print(len(bib_title_vocabulary))\n",
    "print(len(bib_title_vocabulary1))\n",
    "print(len(title_vocabulary))\n",
    "print(len(title_vocabulary1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(bib_venue_vocabulary1, open(DATA+\"_bib_venue_vocabulary1.p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "pickle.dump(bib_title_vocabulary1, open(DATA+\"_bib_title_vocabulary1.p\", \"wb\"))  # save it into a file named save.p\n",
    "\n",
    "\n",
    "pickle.dump(title_vocabulary1, open(DATA+\"_title_vocabulary1.p\", \"wb\"))  # save it into a file named save.p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature vector = title + bib_title+bib_venue\n",
    "\n",
    "def get_feature_vector(x,paper_title,bib_title,bib_venue,prefix_value,feature_vector):\n",
    "    c = 0\n",
    "    for file in x:\n",
    "        prefix = prefix_value+file\n",
    "        c = c + 1\n",
    "        if c%100 ==0 : \n",
    "            print(c)\n",
    "        feature_vector[prefix] = np.concatenate(\n",
    "                                (bagofwords(paper_title[prefix],title_vocabulary1),\n",
    "                                bagofwords(bib_title[prefix],bib_title_vocabulary1),\n",
    "                                bagofwords(bib_venue[prefix],bib_venue_vocabulary1)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n"
     ]
    }
   ],
   "source": [
    "feature_vector = {}\n",
    "x = listdir(bib_text_path_1)\n",
    "get_feature_vector(x,paper_title,bib_title,bib_venue,'pos_',feature_vector)\n",
    "x = listdir(bib_text_path_2)\n",
    "get_feature_vector(x,paper_title,bib_title,bib_venue,'neg_',feature_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(feature_vector, open(DATA+\"_train_only_bib_feature_vector_final.p\", \"wb\"))  # save it into a file named save.p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "bib_text_path_1 = \"/home1/tirthankar/btpfinal/dataset/\"+DATA+\"/test/positive_bib\"\n",
    "bib_text_path_2 = \"/home1/tirthankar/btpfinal/dataset/\"+DATA+\"/test/negative_bib\"\n",
    "\n",
    "#for  jnca get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "x = listdir(bib_text_path_1)\n",
    "prefix = 'pos_'\n",
    "paper_title = {}\n",
    "bib_title = {}\n",
    "bib_venue = {}\n",
    "get_unigrams(x,bib_text_path_1,paper_title,bib_title,bib_venue,prefix)\n",
    "#for comnet get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "x = listdir(bib_text_path_2)\n",
    "prefix = 'neg_'\n",
    "get_unigrams(x,bib_text_path_2,paper_title,bib_title,bib_venue,prefix)\n",
    "\n",
    "\n",
    "feature_vector = {}\n",
    "x = listdir(bib_text_path_1)\n",
    "get_feature_vector(x,paper_title,bib_title,bib_venue,'pos_',feature_vector)\n",
    "x = listdir(bib_text_path_2)\n",
    "get_feature_vector(x,paper_title,bib_title,bib_venue,'neg_',feature_vector)\n",
    "\n",
    "\n",
    "pickle.dump(feature_vector, open(DATA+\"_test_only_bib_feature_vector_final.p\", \"wb\"))  # save it into a file named save.p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
