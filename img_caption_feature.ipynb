{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"ARTINT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 13 11:59:36 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0  On |                  N/A |\n",
      "|  0%   53C    P2    62W / 250W |   4987MiB / 11172MiB |     18%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 28%   49C    P2    59W / 250W |   5221MiB / 11172MiB |     22%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "|  0%   56C    P2    61W / 250W |   4726MiB / 11172MiB |     20%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 29%   51C    P2    80W / 250W |   6105MiB / 11172MiB |     79%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce GTX 108...  Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 30%   60C    P2    63W / 250W |   8506MiB / 11172MiB |     19%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce GTX 108...  Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 23%   39C    P8    17W / 250W |    706MiB / 11172MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce GTX 108...  Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 27%   47C    P2    58W / 250W |   2302MiB / 11172MiB |     20%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce GTX 108...  Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 29%   58C    P2   221W / 250W |   6772MiB / 11172MiB |     19%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      4670      G   /usr/lib/xorg/Xorg                            50MiB |\n",
      "|    0     14467      C   python                                      4715MiB |\n",
      "|    0     24425      C   ...1/tanik/anaconda3/envs/tfgpu/bin/python   209MiB |\n",
      "|    1     21260      C   python                                      4715MiB |\n",
      "|    1     46423      C   ...1/tanik/anaconda3/envs/tfgpu/bin/python   495MiB |\n",
      "|    2     15533      C   python                                      4715MiB |\n",
      "|    3     22182      C   python                                      2291MiB |\n",
      "|    3     22346      C   python                                      1267MiB |\n",
      "|    3     22551      C   python                                      1267MiB |\n",
      "|    3     22758      C   python                                      1267MiB |\n",
      "|    4     24203      C   python                                      8495MiB |\n",
      "|    5     13418      C   /home1/tirthankar/miniconda2/bin/python      695MiB |\n",
      "|    6     35449      C   python                                      2291MiB |\n",
      "|    7      3392      C   python                                      6761MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_path_1 = DATA+\"train/positive_image_caption\"\n",
    "img_text_path_2 = DATA+\"/train/negative_image_caption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    words = []  \n",
    "    w = word_tokenize(sentence)\n",
    "    w = [word.lower() for word in w]\n",
    "    w = [word for word in w if word.isalpha()]\n",
    "    stop_words = stopwords.words('english')\n",
    "    w = [word for word in w if not word in stop_words]\n",
    "    words.extend(w)\n",
    "    return words\n",
    "\n",
    "    \n",
    "def bagofwords(sentence, words):\n",
    "    bag = np.zeros(len(words))\n",
    "    for sw in sentence:\n",
    "        for i,word in enumerate(words):\n",
    "            if word == sw: \n",
    "                bag[i] = 1                \n",
    "    return np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams(x,path,image_caption,prefix):\n",
    "\n",
    "    for file in x:\n",
    "        key = prefix+file[:len(file)-5]+\".pdf.json\"\n",
    "        image_caption[key] = \"\"\n",
    "        with open(join(path,file),encoding=\"utf8\") as input_file:\n",
    "            data = json.load(input_file)\n",
    "            for x in range(len(data)):\n",
    "                if data[x]['caption'] is not None:\n",
    "                    image_caption[key] = image_caption[key] + \"\".join(data[x]['caption'].split('.')[2:])\n",
    "               \n",
    "            image_caption[key] = preprocess(image_caption[key])\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for  jnca get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "x = listdir(img_text_path_1)\n",
    "prefix = 'pos_'\n",
    "image_caption = {}\n",
    "get_unigrams(x,img_text_path_1,image_caption,prefix)\n",
    "#for comnet get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "x = listdir(img_text_path_2)\n",
    "prefix = 'neg_'\n",
    "get_unigrams(x,img_text_path_2,image_caption,prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocabulary(vocabulary_name,document_unigram,prefix,x):\n",
    "    for file in x:\n",
    "        vocabulary_name = vocabulary_name + document_unigram[prefix+file[:len(file)-5]+\".pdf.json\"]\n",
    "    return vocabulary_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_caption_vocabulary = []\n",
    "\n",
    "x = listdir(img_text_path_1)\n",
    "img_caption_vocabulary = generate_vocabulary(img_caption_vocabulary,image_caption,'pos_',x)\n",
    "\n",
    "x = listdir(img_text_path_2)\n",
    "img_caption_vocabulary = generate_vocabulary(img_caption_vocabulary,image_caption,'neg_',x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_caption_vocabulary1 = sorted(list(set(img_caption_vocabulary)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n"
     ]
    }
   ],
   "source": [
    "remove_list = []\n",
    "c = 0\n",
    "for i in set(img_caption_vocabulary1):\n",
    "    c = c+ 1\n",
    "    if c %100 == 0:\n",
    "        print(c)\n",
    "    if img_caption_vocabulary.count(i) < 3:\n",
    "         img_caption_vocabulary1.remove(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137616\n",
      "5716\n"
     ]
    }
   ],
   "source": [
    "print(len(img_caption_vocabulary))\n",
    "print(len(img_caption_vocabulary1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(img_caption_vocabulary1, open(DATA+\"_image_caption_vocabulary_final.p\", \"wb\"))  # save it into a file named save.p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature vector = title + bib_title+bib_venue\n",
    "\n",
    "def get_feature_vector(x,img_caption,prefix_value,feature_vector):\n",
    "\n",
    "    for file in x:\n",
    "        prefix = prefix_value+file[:len(file)-5]+\".pdf.json\"\n",
    "        feature_vector[prefix] = bagofwords(img_caption[prefix],img_caption_vocabulary1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = {}\n",
    "x = listdir(img_text_path_1)\n",
    "get_feature_vector(x,image_caption,'pos_',feature_vector)\n",
    "x = listdir(img_text_path_2)\n",
    "get_feature_vector(x,image_caption,'neg_',feature_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(feature_vector, open(\"JNCA_train_image_caption_feature_vector.p\", \"wb\"))  # save it into a file named save.p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_text_path_1 = DATA+\"/test/positive_image_caption\"\n",
    "img_text_path_2 = DATA+\"/test/negative_image_caption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = {}\n",
    "#for  jnca get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "x = listdir(img_text_path_1)\n",
    "prefix = 'pos_'\n",
    "image_caption = {}\n",
    "get_unigrams(x,img_text_path_1,image_caption,prefix)\n",
    "#for comnet get unigrams or each element is dictionary of prefix_filename and corresponding unigram words preprocessed\n",
    "x = listdir(img_text_path_2)\n",
    "prefix = 'neg_'\n",
    "get_unigrams(x,img_text_path_2,image_caption,prefix)\n",
    "\n",
    "\n",
    "x = listdir(img_text_path_1)\n",
    "get_feature_vector(x,image_caption,'pos_',feature_vector)\n",
    "x = listdir(img_text_path_2)\n",
    "get_feature_vector(x,image_caption,'neg_',feature_vector)\n",
    "\n",
    "pickle.dump(feature_vector, open(DATA+\"_test_image_caption_feature_vector.p\", \"wb\"))  # save it into a file named save.p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
