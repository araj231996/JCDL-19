{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import layers\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"STATPRO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'STATPRO_train_img_filename_for_img_modal_vector.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-85801bec60a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_img_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_train_img_filename_for_img_modal_vector.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save it into a file named save.p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_img_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_test_img_filename_for_img_modal_vector.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# save it into a file named save.p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_img_modal_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_train_img_modal_vector.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_img_modal_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_test_img_modal_vector.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'STATPRO_train_img_filename_for_img_modal_vector.p'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_img_file = pickle.load(open(DATA+\"_train_img_filename_for_img_modal_vector.p\", \"rb\"))  # save it into a file named save.p\n",
    "test_img_file = pickle.load( open(DATA+\"_test_img_filename_for_img_modal_vector.p\", \"rb\"))  # save it into a file named save.p\n",
    "train_img_modal_vector = np.load(DATA+\"_train_img_modal_vector.npy\")\n",
    "test_img_modal_vector = np.load(DATA+\"_test_img_modal_vector.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in train_img_file:\n",
    "    if 'pos_' in i:\n",
    "        y.append(1.0)\n",
    "    else:\n",
    "        y.append(0.0)\n",
    "        \n",
    "y_test = []\n",
    "for i in test_img_file:\n",
    "    if 'pos_' in i:\n",
    "        y_test.append(1.0)\n",
    "    else:\n",
    "        y_test.append(0.0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                204850    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 210,001\n",
      "Trainable params: 210,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3064 samples, validate on 769 samples\n",
      "Epoch 1/10\n",
      "3064/3064 [==============================] - 8s 3ms/step - loss: 0.6393 - acc: 0.6335 - val_loss: 0.6022 - val_acc: 0.6983\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60218, saving model to model-best.h5\n",
      "Epoch 2/10\n",
      "3064/3064 [==============================] - 0s 95us/step - loss: 0.6172 - acc: 0.6687 - val_loss: 0.5892 - val_acc: 0.7217\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60218 to 0.58916, saving model to model-best.h5\n",
      "Epoch 3/10\n",
      "3064/3064 [==============================] - 0s 89us/step - loss: 0.6035 - acc: 0.6948 - val_loss: 0.5688 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.58916 to 0.56882, saving model to model-best.h5\n",
      "Epoch 4/10\n",
      "3064/3064 [==============================] - 0s 101us/step - loss: 0.5962 - acc: 0.6968 - val_loss: 0.5681 - val_acc: 0.7321\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56882 to 0.56812, saving model to model-best.h5\n",
      "Epoch 5/10\n",
      "3064/3064 [==============================] - 0s 94us/step - loss: 0.5941 - acc: 0.6939 - val_loss: 0.5633 - val_acc: 0.7334\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.56812 to 0.56328, saving model to model-best.h5\n",
      "Epoch 6/10\n",
      "3064/3064 [==============================] - 0s 93us/step - loss: 0.5946 - acc: 0.6945 - val_loss: 0.5635 - val_acc: 0.7243\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.56328\n",
      "Epoch 7/10\n",
      "3064/3064 [==============================] - 0s 98us/step - loss: 0.5932 - acc: 0.6961 - val_loss: 0.5709 - val_acc: 0.7295\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.56328\n",
      "Epoch 8/10\n",
      "3064/3064 [==============================] - 0s 93us/step - loss: 0.5897 - acc: 0.6984 - val_loss: 0.5741 - val_acc: 0.7321\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.56328\n",
      "Epoch 9/10\n",
      "3064/3064 [==============================] - 0s 92us/step - loss: 0.5880 - acc: 0.6984 - val_loss: 0.5601 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.56328 to 0.56011, saving model to model-best.h5\n",
      "Epoch 10/10\n",
      "3064/3064 [==============================] - 0s 99us/step - loss: 0.5910 - acc: 0.6965 - val_loss: 0.5666 - val_acc: 0.7256\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.56011\n"
     ]
    }
   ],
   "source": [
    "# Input - Layer\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_dim=4096))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "model.summary()\n",
    "checkpoint = ModelCheckpoint(DATA+'model-best.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  \n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "results = model.fit( train_img_modal_vector,y, epochs= 10, batch_size = 64,validation_data=(test_img_modal_vector,y_test),callbacks=[checkpoint])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted =  model.predict(test_img_modal_vector)\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i] >= 0.5:\n",
    "        predicted[i] = 1.0\n",
    "    else :\n",
    "        predicted[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fscore 0.725587987086016\n",
      "precision 0.7265787905868335\n",
      "recall 0.7269598535195985\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.75      0.72       365\n",
      "        1.0       0.76      0.70      0.73       404\n",
      "\n",
      "avg / total       0.73      0.73      0.73       769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"fscore\",f1_score(y_test, predicted, average=\"macro\"))\n",
    "print(\"precision\",precision_score(y_test, predicted, average=\"macro\"))\n",
    "print(\"recall\",recall_score(y_test, predicted, average=\"macro\"))    \n",
    "\n",
    "x = classification_report(y_test,predicted)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(DATA+'_only_image_dnn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:raghav_btp]",
   "language": "python",
   "name": "conda-env-raghav_btp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
